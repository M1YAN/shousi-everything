{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4101a887",
   "metadata": {},
   "source": [
    "# 归一化层（Normalization）\n",
    "\n",
    "归一化层能有效稳定训练过程，加速模型收敛。\n",
    "\n",
    "## Layer Normalization (LN)\n",
    "\n",
    "- 核心思想：在单个样本内部进行归一化。对于一个样本，会计算其所有特征维度的均值和方差，然后用它来归一化该样本。\n",
    "- 公式：$\\text{LN}(x) = \\gamma \\odot \\frac{x-\\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$\n",
    "- 应用：由 `normalized_shape` 参数决定\n",
    "  - NLP 模型：输入是 `(batch_size, seq_len, embedding_dim)`，`normalized_shape` 是 `(embedding_dim, )`，在最后一个维度上做归一化\n",
    "  - 图像模型：输入是 `(batch_size, channels, height, width)`，`normalized_shape` 可能设置成 `(channels, height, width)`，此时在所有特征维度上做归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad4da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miyan/miniconda3/envs/torch/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# Layer Normalization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
    "        '''\n",
    "        Args:\n",
    "            normalized_shape: 需要归一化的维度\n",
    "            eps: 防止除零\n",
    "            elementwise_affine: 是否使用可学习的平移和缩放参数\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            self.gamma = nn.Parameter(torch.ones(self.normalized_shape)) # 可学习的缩放参数，初始为1\n",
    "            self.beta = nn.Parameter(torch.zeros(self.normalized_shape)) # 可学习的平移参数，初始为0\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 确认张量最后维度与 normalized_shape 是对应的\n",
    "        assert x.shape[-len(self.normalized_shape):] == self.normalized_shape\n",
    "\n",
    "        # 获取需要归一化的维度索引\n",
    "        # e.g. NLP (batch, seq_len, embedding_dim), normalized shape: (embedding_dim, ), dims = (-1, 0)\n",
    "        dims = tuple(range(-len(self.normalized_shape), 0))\n",
    "\n",
    "        mean = x.mean(dim=dims, keepdim=True)\n",
    "        var = x.var(dim=dims, keepdim=True, unbiased=False) # unbiased=False 代表计算整体方差（分母是 N，不是 N-1）\n",
    "\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            return self.gamma * x_norm + self.beta\n",
    "        \n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1d17dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NLP input ===\n",
      "input shape:  torch.Size([2, 4, 8])\n",
      "normalized shape:  (8,)\n",
      "output shape:  torch.Size([2, 4, 8])\n",
      "output:  tensor([[[-1.4625, -0.6196,  1.5962,  0.6424, -0.2388, -0.7670, -0.4443,\n",
      "           1.2937],\n",
      "         [-0.4792,  1.0489,  0.9811,  0.4844, -0.5933, -1.5518, -1.1085,\n",
      "           1.2183],\n",
      "         [-0.1809, -1.2149, -0.4875,  0.9943,  0.2714, -1.6061,  1.3757,\n",
      "           0.8480],\n",
      "         [-1.0473,  0.7146,  2.0784, -0.3647,  0.1784, -0.1274, -0.0579,\n",
      "          -1.3741]],\n",
      "\n",
      "        [[ 1.3167,  1.1966,  0.6393, -0.6592, -0.2899, -1.1827,  0.4865,\n",
      "          -1.5072],\n",
      "         [ 0.4824,  1.1537,  1.1923, -0.8763, -0.6078,  0.8967, -1.6507,\n",
      "          -0.5904],\n",
      "         [-0.6461, -0.0729, -0.3627, -0.8746,  1.2042,  1.1846, -1.5826,\n",
      "           1.1500],\n",
      "         [-1.1200,  1.8385,  0.1356,  0.0567, -1.0411,  0.8595,  0.4275,\n",
      "          -1.1568]]], grad_fn=<AddBackward0>)\n",
      "=== Vision input ===\n",
      "input shape:  torch.Size([2, 3, 4, 4])\n",
      "normalized shape:  (3, 4, 4)\n",
      "output shape:  torch.Size([2, 3, 4, 4])\n",
      "output:  tensor([[[[ 0.3049,  0.0549, -0.4979, -1.0835],\n",
      "          [-0.5775, -0.3523, -0.2689,  0.2402],\n",
      "          [ 1.1156,  0.0584, -3.5300,  1.1596],\n",
      "          [ 0.3243,  0.6752, -0.3421,  0.8914]],\n",
      "\n",
      "         [[ 1.6789,  0.0380,  1.4406,  1.1916],\n",
      "          [-0.4909, -0.3251, -0.2662,  1.3158],\n",
      "          [ 1.6255,  0.1061,  0.7855, -0.4291],\n",
      "          [-0.9746, -0.1755,  0.3217,  0.1283]],\n",
      "\n",
      "         [[-1.9371,  1.1320,  0.6928,  0.2136],\n",
      "          [ 1.0500, -0.3775, -0.5379, -2.0363],\n",
      "          [ 0.6806,  0.0162, -0.1044, -1.5190],\n",
      "          [ 0.0746,  0.0449, -0.0126, -1.5227]]],\n",
      "\n",
      "\n",
      "        [[[-1.2222, -1.3630,  1.8585, -0.1644],\n",
      "          [-0.4067,  0.4643,  1.5909, -0.6552],\n",
      "          [ 0.1087,  0.2519, -2.1329,  1.3232],\n",
      "          [-1.1153,  0.1106,  0.1264,  0.5644]],\n",
      "\n",
      "         [[ 0.4658,  0.7001,  0.8449, -2.6718],\n",
      "          [-0.0960,  0.0976,  0.5254,  0.6072],\n",
      "          [ 0.3721,  0.1167, -0.3107, -1.4205],\n",
      "          [-0.1548, -0.8913, -0.2307, -0.1781]],\n",
      "\n",
      "         [[ 0.0934,  0.3031,  0.3756,  1.8773],\n",
      "          [-0.2846, -1.4208,  0.7276, -0.5604],\n",
      "          [ 0.8337, -0.2385, -0.1219, -0.7960],\n",
      "          [ 1.1482,  0.5543,  2.1533, -1.7592]]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Use Layer Normalization\n",
    "batch_size, seq_len, d_model = 2, 4, 8\n",
    "\n",
    "x_nlp = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "ln_nlp = LayerNorm(normalized_shape=(d_model, ))\n",
    "\n",
    "output_nlp = ln_nlp(x_nlp)\n",
    "\n",
    "print(\"=== NLP input ===\")\n",
    "print(\"input shape: \", x_nlp.shape)\n",
    "print(\"normalized shape: \", ln_nlp.normalized_shape)\n",
    "print(\"output shape: \", output_nlp.shape)\n",
    "print(\"output: \", output_nlp)\n",
    "\n",
    "batch_size, channels, height, width = 2, 3, 4, 4\n",
    "\n",
    "x_vision = torch.randn(batch_size, channels, height, width)\n",
    "\n",
    "# 对最后三个维度做归一化（所有特征）(channels, height, width)\n",
    "\n",
    "ln_vision = LayerNorm(normalized_shape=(channels, height, width))\n",
    "\n",
    "output_vision = ln_vision(x_vision)\n",
    "\n",
    "print(\"=== Vision input ===\")\n",
    "print(\"input shape: \", x_vision.shape)\n",
    "print(\"normalized shape: \", ln_vision.normalized_shape)\n",
    "print(\"output shape: \", output_vision.shape)\n",
    "print(\"output: \", output_vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e84bea",
   "metadata": {},
   "source": [
    "## RMSNorm (Root Mean Square Layer Normalization)\n",
    "\n",
    "- 核心思想：是 Layer Normalization 的简化版，被 Llama 等模型采用。移除了均值中心化操作（减去均值 $\\mu$），只进行方差的缩放，计算量更小\n",
    "- 计算方法：对于给定的输入 X，X 是一个 $n \\times d$ 的矩阵，$n$ 是批量大小，$d$是特征维度，RMSNorm 的计算可以表示为\n",
    "    1. 计算每个样本的特征平方的均方根：\n",
    "        $$\n",
    "        \\mu = \\frac{1}{d}\\sum_{i = 1}^d x_i^2\n",
    "        $$\n",
    "    2. 接着计算均方根的倒数，加上一个小常数防止除零：\n",
    "        $$\n",
    "        \\text{RMS} = \\sqrt{\\frac{1}{\\mu + \\sigma}}\n",
    "        $$\n",
    "    3. 使用得到的 RMS 值对输入进行归一化，并乘可学习的权重参数 $\\omega$：\n",
    "        $$\n",
    "        Y = X * \\text{RMS} * \\omega\n",
    "        $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd893b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 1. 计算平方的均值\n",
    "        mean_of_square = x.pow(2).mean(-1, keepdim=True)\n",
    "\n",
    "        # 2. 计算均方根的倒数\n",
    "        rrms = torch.rsqrt(mean_of_square + self.eps)\n",
    "\n",
    "        return rrms * x\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = self._norm(x.float()).type_as(x) # 中间结果使用 float32 保证精度，然后转回原始类型\n",
    "        return output * self.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
