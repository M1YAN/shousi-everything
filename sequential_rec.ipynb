{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2de1af8",
   "metadata": {},
   "source": [
    "# 序列推荐 (Sequential Recommendation)\n",
    "\n",
    "## DIN (Deep Interest Network)\n",
    "\n",
    "- 核心原理：在预测用户是否会点击一个 target item 时，用户的兴趣不应该是一个固定的向量，而应该根据这个目标商品来动态调整。\n",
    "- DIN 通过一个注意力机制 (Attention Unit) 来实现这种“局部激活” (Local Activation) 的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Attention Unit\n",
    "class AttentionUnit(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, hidden_units):\n",
    "        super(AttentionUnit, self).__init__()\n",
    "        # fc1 的输入维度是 query_dim + key_dim\n",
    "        # 将目标商品和历史商品拼在一起\n",
    "        self.fc1 = nn.Linear(query_dim + key_dim, hidden_units)\n",
    "        # fc2 输出一个单独的“相关性分数”\n",
    "        self.fc2 = nn.Linear(hidden_units, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        # query: (batch_size, embedding_size)\n",
    "        # 目标商品的 embedding\n",
    "\n",
    "        # keys: (batch_size, seq_len, embedding_size)\n",
    "        # 历史行为序列的 embedding\n",
    "        seq_len = keys.size(1)\n",
    "\n",
    "        # 1. 扩展 query\n",
    "        # 将目标商品扩展 seq_len 份，使其维度与 key 相同\n",
    "        # Why？ 为了计算目标商品与每一个历史商品的相关性\n",
    "        queries = query.unsqueeze(1).expand(-1, seq_len, -1)  # (batch_size, seq_len, embedding_size)\n",
    "\n",
    "        # 2. 拼接 query 和 keys\n",
    "        # 这是 DIN 注意力机制的特点：显式地将 query 和 keys 拼接起来\n",
    "        inputs = torch.cat([queries, keys], dim = -1) # shape: (batch, seq_len, embedding_size * 2)\n",
    "\n",
    "        # 3. 通过 MLP 计算“相关性”\n",
    "        # “注意力网络” 是通过 MLP 架构实现的\n",
    "        out = F.relu(self.fc1(inputs)) # out shape: (batch_size, seq_len, hidden_units)\n",
    "\n",
    "        # 得到每个历史商品的原始注意力分数\n",
    "        out = self.fc2(out).squeeze(-1) # shape: (batch_size, seq_len）each batch: (seq_len, ), single original score\n",
    "\n",
    "        # 4. Softmax 归一化\n",
    "        attention_weights = F.softmax(out, dim=-1) # shape: (batch_size, seq_len)\n",
    "\n",
    "        return attention_weights\n",
    "\n",
    "\n",
    "# DIN implementation\n",
    "\n",
    "class DIN(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim, attention_hidden_units):\n",
    "        super(DIN, self).__init__()\n",
    "\n",
    "        # 1. Embedding 层，存储 user id 和 item id 的 embedding\n",
    "        self.user_embedding = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(item_num, embedding_dim)\n",
    "\n",
    "        # 2. 实例化 Attention 单元\n",
    "        self.attention = AttentionUnit(embedding_dim, embedding_dim, attention_hidden_units)\n",
    "\n",
    "        # 3. 顶层 DNN (Deep Neural Network)\n",
    "        # 注意：输入维度是 embedding_dim * 3\n",
    "        self.dnn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 3, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(80, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(40,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, user, hist_items, target_item):\n",
    "        # == embedding lookup ==\n",
    "        # 静态用户特征\n",
    "        user_emb = self.user_embedding(user) # shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # 目标商品特征\n",
    "        target_item_emb = self.item_embedding(target_item) # shape: (batch_szie, embedding_dim)\n",
    "\n",
    "        # 历史商品序列\n",
    "        hist_item_emb = self.item_embedding(hist_items) # shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # == Core Steps ==\n",
    "\n",
    "        # 1. 计算注意力分数\n",
    "        attention_weights = self.attention(target_item_emb, his_items_emb) # shape: (batch_size, seq_len)\n",
    "\n",
    "        # 2. 应用注意力权重（加权求和）\n",
    "        # attention_weights: (batch, seq_len）-> (batch, seq_len, 1)\n",
    "        # hist_item_emb: (batch, seq_len, embedding_dim)\n",
    "        # 沿着 seq_len 维度求和，得到一个 (batch, embedding_dim) 的张量，一个 embedding 代表一个历史序列\n",
    "        weighted_hist_emb = torch.sum(hist_item_emb, attention_weights.unsqueeze(-1), dim = 1)\n",
    "\n",
    "        # weighted_hist_emb 是 **动态**，**为 target item** 定制的 embedding，融合了当前 item 和用户兴趣\n",
    "\n",
    "        # == Concat & Prediction\n",
    "\n",
    "        # 1. 拼接所有特征\n",
    "        # 拼接 user_emb（静态用户特征）, target_item_emb（目标商品特征） ,weighted_hist_emb（动态用户兴趣）\n",
    "        dnn_input = torch.cat([user_emb, target_item_emb, weighted_hist_emb], dim=-1) # shape: (batch, embedding_dim * 3)\n",
    "\n",
    "        # 2. 通过 DNN 预测\n",
    "        output = self.dnn(dnn_input) # shape: (batch, 1)\n",
    "\n",
    "        # 3. Sigmoid 输出，作为点击率的预测值\n",
    "        return torch.sigmoid(output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
