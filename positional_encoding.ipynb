{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef8e800",
   "metadata": {},
   "source": [
    "# 位置编码 (Positional Embedding)\n",
    "\n",
    "标准 tranformer 架构的自注意力机制本身是“置换不变”的，也就是说无法感知输入序列中 token 的顺序。为了解决这个问题，必须向模型中注入关于 token 位置的信息，这就是位置编码的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8915e9",
   "metadata": {},
   "source": [
    "## 旋转位置编码 (Rotary Position Encoding, RoPE)\n",
    "\n",
    "- 核心思想：不再将位置信息加在 embedding 中，而是通过数学上的旋转操作在融合位置信息。具体来说，根据 token 的绝对位置，对 Q 和 K 向量在二维子空间中进行旋转\n",
    "- 关键性质：经过 RoPE 旋转的向量（$Q_m, K_n$，分别在位置$m$和$n$），它们的内积结果中只包含了它们的相对位置 $m - n$，而绝对位置被消除了。使得注意力机制能够天然地关注到相对位置\n",
    "- 实现方式：\n",
    "    1. 复数形式：对于位置为 $m$ 的 token，其 query $q$ 经过 RoPE 变换后：\n",
    "   $$\n",
    "   f(q, m) = q * e^{i * m * \\theta}\n",
    "   $$\n",
    "   其中：\n",
    "   - $\\theta = 10000^{-2k/d}$ 是频率参数\n",
    "   - k 是索引维度，d 是向量维度\n",
    "   - i 是虚数单位\n",
    "    2. 矩阵形式\n",
    "    ```python\n",
    "    R_m = [[cos(m*theta), -sin(m*theta)],\n",
    "            [sin(m*theta), cos(m*theta)]]\n",
    "\n",
    "    # 对相邻两个维度应用RoPE\n",
    "    [q1, q2] = R_m @ [q1, q2]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63d45765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import typing\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    '''\n",
    "    Args:\n",
    "        dim (int): 头的维度（必须为偶数）\n",
    "        end (int)：序列最大长度\n",
    "        theta (float)：RoPE 的基数，一般为 10000.0\n",
    "    Return:\n",
    "        torch.Tensor: 形状为 (end, dim // 2) 的复数张量\n",
    "    '''\n",
    "    # freq for each dimension is 1 / (theta^(2i/dim)), shape: (dim // 2, )\n",
    "    freq_base = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "\n",
    "    # create timesteps, shape: (end, )\n",
    "    t = torch.arange(end, dtype=torch.float32)\n",
    "\n",
    "    # calculate rotary angle, shape: (end, dim // 2)\n",
    "    freqs = torch.outer(t, freq_base)\n",
    "\n",
    "    # 将频率（角度）变成复数形式 cos(freqs) + i*sin(freqs)\n",
    "    # torch.polar(abs, angle) -> abs * (cos(angle) + i * sin(angle))\n",
    "    freq_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freq_cis\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "    '''\n",
    "    Args:\n",
    "        x (torch.Tensor): input Q or K, shape: (batch, num_heads, seq_len, d_k)\n",
    "        freq_cis (torch.Tensor): shape: (seq_len, d_model // 2)\n",
    "    Return:\n",
    "        torch.Tensor: shape: similar to x\n",
    "    '''\n",
    "    # 1. 将 x 的最后一个维度看作 D//2 个复数，x: (..., D) -> (..., D//2, 2)\n",
    "    x_reshaped = x.float().reshape(*x.shape[: -1], -1, 2)\n",
    "    # x_complex: (..., D//2)\n",
    "    x_complex = torch.view_as_complex(x_reshaped)\n",
    "\n",
    "    # 2. 调整 freqs_cis 的形状以进行广播，(seq_len, D//2) -> (1, 1, seq_len, D // 2)\n",
    "    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "    # 3. 执行复数乘法，实现旋转\n",
    "    # (batch, num_heads, seq_len, D // 2) * (1, 1, seq_len, D // 2)\n",
    "    x_rotated = x_complex * freqs_cis\n",
    "\n",
    "    # 4. 转换为实数形式\n",
    "    # (..., D // 2) -> (..., D // 2, 2)\n",
    "    x_out_reshaped = torch.view_as_real(x_rotated)\n",
    "    # (..., D // 2, 2) -> (..., D)\n",
    "    x_out = x_out_reshaped.flatten(3)\n",
    "\n",
    "    return x_out.type_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9d76eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  1.0000,   2.0000,   3.0000,   4.0000],\n",
      "          [ -2.3473,   7.4492,   6.9197,   8.0696],\n",
      "          [-12.8383,   4.0222,  10.7578,  12.2176]]]])\n"
     ]
    }
   ],
   "source": [
    "# test rotary embedding\n",
    "\n",
    "# input tensor\n",
    "x = torch.tensor([[[[1.0, 2.0, 3.0, 4.0],\n",
    "                    [5.0, 6.0, 7.0, 8.0],\n",
    "                    [9.0, 10.0, 11.0, 12.0]]]])  # shape: (1, 1, 3, 4)\n",
    "# precompute freqs_cis\n",
    "freqs_cis = precompute_freqs_cis(dim=4, end=3)\n",
    "# apply rotary embedding\n",
    "x_rotated = apply_rotary_emb(x, freqs_cis)\n",
    "print(x_rotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843dd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
