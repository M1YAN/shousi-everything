{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abddb067",
   "metadata": {},
   "source": [
    "# 强化学习\n",
    "\n",
    "## 核心概念与公式\n",
    "\n",
    "- 贝尔曼方程 (Bellman Equation)：定义了当前状态（或状态-动作对）的价值与后续状态价值之间的关系，是多数 RL 算法的基础。\n",
    "  - 状态价值函数\n",
    "    $$\n",
    "    V^{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma V^{\\pi}(S_{t+1})|S_t = s]\n",
    "    $$\n",
    "  - 动作价值函数\n",
    "    $$\n",
    "    Q^{\\pi}(s, a) = r(s, a) + \\gamma \\sum_{s'}p(s' | s, a)V^{\\pi}(s')\n",
    "    $$\n",
    "- Q-learning：一种经典的 off-policy 算法，直接学习最优动作价值函数 $Q^*(s, a)$。\n",
    "  - 更新规则：$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha[r_t + \\gamma \\mathop{max}\\limits_{a} Q(s_{t+1},a) - Q(s_t, a_t)]$\n",
    "- 策略梯度：直接对策略 $\\pi_\\theta(a|s)$ 进行优化。目标是找到一组参数 $\\theta$，是的期望回报最大化。\n",
    "  - 梯度公式：$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[Q^{\\pi_\\theta}(s, a)\\nabla_\\theta \\text{log}\\pi_\\theta(a|s)] $\n",
    "- PPO (Proximal Policy Optimization)：目前 RLHF 中最主流的策略梯度算法，通过一个截断 (clipping) 的目标函数来限制每次策略更新的幅度，保证训练的稳定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ca352",
   "metadata": {},
   "source": [
    "## PPO (Proximal Policy Optimization)\n",
    "\n",
    "PPO 的核心是在最大化期望回报的同时，避免策略更新过大导致训练崩溃。\n",
    "\n",
    "- 目标函数：\n",
    "  \n",
    "    $$\n",
    "    L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t[\\text{min}(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t)] \\\\\n",
    "    r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\n",
    "    $$\n",
    "\n",
    "    $\\hat{A}_t$ 是优势函数 (Advantage Function)，表示在状态 $s_t$ 下选择动作 $a_t$ 比平均水平好多少。\n",
    "\n",
    "    `clip`函数将概率比限制在 $[1 - \\epsilon, 1 + \\epsilon]$ 区间内。\n",
    "\n",
    "- 工作机制：\n",
    "\n",
    "    - 如果优势 $\\hat{A}_t > 0$（好动作），目标函数鼓励增大 $r_t(\\theta)$，但上限是$1 + \\epsilon$。\n",
    "    - 如果优势 $\\hat{A}_t < 0$（坏动作），目标函数鼓励减小 $r_t(\\theta)$，但下限是$1 - \\epsilon$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Loss (Actor Loss)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class PolicyLoss(nn.Module):\n",
    "    '''\n",
    "    PPO Policy Loss\n",
    "    '''\n",
    "    def __init__(self, clip_eps: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.clip_eps = clip_eps\n",
    "\n",
    "    def forward(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor, advantages: torch.Tensor) ->torch.Tensor:\n",
    "        # 1. 计算新旧策略的概率比\n",
    "        ratio = (log_probs - old_log_probs).exp()\n",
    "\n",
    "        # 2. 计算两个目标项\n",
    "        # 未截断的目标\n",
    "        surr1 = ratio * advantages\n",
    "        # 截断后的目标\n",
    "        surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages\n",
    "\n",
    "        # 3. 取两者中较小的一个，并且加上负号（取最小值是为了防止过大的更新）\n",
    "        return -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "# Value Loss (Critic Loss)\n",
    "class ValueLoss(nn.Module):\n",
    "    '''\n",
    "    PPO Value Loss\n",
    "    '''\n",
    "    def __init__(self, clip_eps: float = None):\n",
    "        super().__init__()\n",
    "        self.clip_eps = clip_eps\n",
    "\n",
    "    def forward(self, values: torch.Tensor, old_values: torch.Tensor, returns: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            values: 当前 Critic 网络的价值预测 V(s_t)\n",
    "            old_values: 生成经验时旧 Critic 网络的价值预测\n",
    "            return: 实际回报\n",
    "        '''\n",
    "        # 计算原始的均方误差损失\n",
    "        loss = (values - returns) ** 2\n",
    "\n",
    "        # Optional：对 value 也进行 clip，防止其变化过大\n",
    "        if self.clip_eps is not None:\n",
    "            values_clipped = old_values + torch.clamp(values - old_values, -self.clip_eps, self.clip_eps)\n",
    "            loss_clipped = (values_clipped - returns) ** 2\n",
    "            loss = torch.max(loss, loss_clipped)\n",
    "\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb482843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整 PPO 训练循环\n",
    "class PPOTrainer:\n",
    "    def __init__(self):\n",
    "        self.policy_loss_fn = PolicyLoss(clip_eps=0.2)\n",
    "        self.policy_loss_fn = ValueLoss(clip_eps=0.2)\n",
    "\n",
    "    def compute_total_loss(self, batch):\n",
    "        # Actor loss\n",
    "        policy_loss = self.policy_loss_fn(\n",
    "            batch['log_probs'], # 现在策略\n",
    "            batch['old_log_probs'], # 原有策略\n",
    "            batch['advantages'] # 优势值\n",
    "        )\n",
    "\n",
    "        # Critic loss\n",
    "        value_loss = self.value_loss_fn(\n",
    "            batch['values'], # 当前 critic 网络预测的 value\n",
    "            batch['old_values'], # 旧 critic 网络预测的 value\n",
    "            batch['returns'] # 实际回报\n",
    "        )\n",
    "\n",
    "        # total loss\n",
    "        total_loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "        return{\n",
    "            'total_loss': total_loss,\n",
    "            'policy_loss': policy_loss,\n",
    "            'value_loss': value_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da56344",
   "metadata": {},
   "source": [
    "## DPO (Direct Preference Optimization)\n",
    "\n",
    "DPO 是一种绕过显示奖励建模，直接根据偏好数据来优化 LLM 的方法。比传统的 RLHF 流程更加简单稳定。\n",
    "\n",
    "- 核心思想：\n",
    "  - DPO 的目标函数源于一个数学推导，表明标准的 RLHF 优化目标等价表示为一个二元交叉熵损失。这个损失函数的目标是最大化模型对“更优”回答的偏好，同时最小化对“更差”回答的偏好。\n",
    "  - 损失函数：\n",
    "    $$\n",
    "    \\mathcal{L}_{\\text{DPO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = - \\mathbb{E}_{(x, y_w, y_l)\\sim D}[\\text{log} \\sigma(\\beta \\text{log} \\frac{\\pi_\\theta(y_w | x)}{\\pi_{\\text{ref}}(y_w | x)} - \\beta \\text{log} \\frac{\\pi_\\theta(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)})]\n",
    "    $$\n",
    "    - $y_w$ 是偏好的回答，$y_l$ 是不被偏好的回答\n",
    "    - $\\pi_\\theta$ 是正在训练的模型，$\\pi_{\\text{ref}}$ 是固定的参考模型（第一阶段 SFT 训练出的模型）\n",
    "    - $\\beta$ 是控制 KL 散度强度的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814b403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO Loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DPOLoss(nn.Module):\n",
    "    def __init__(self, beta: float = 0.1, label_smoothing: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self,\n",
    "                policy_chosen_logps,\n",
    "                policy_rejected_logps,\n",
    "                reference_chosen_logps,\n",
    "                reference_rejected_logps):\n",
    "        \n",
    "        # 计算策略模型和参考模型在 chosen / rejected 序列上的对数概率比\n",
    "        pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "        ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "\n",
    "        # DPO core\n",
    "        logits = pi_logratios - ref_logratios\n",
    "\n",
    "        # calculate loss\n",
    "        losses = - F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing) - F.logsigmoid(-self.beta * logits) * self.label_smoothing\n",
    "        \n",
    "        # 计算隐式奖励，用于监控\n",
    "        chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach()\n",
    "        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach()\n",
    "\n",
    "        return losses.mean(), chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20376d78",
   "metadata": {},
   "source": [
    "## GRPO (Grouped RL with Policy Optimization)\n",
    "\n",
    "### 核心原理\n",
    "1. 优势估计采用分组奖励归一化 (Grouped Reward Normalization)\n",
    "    在 RLHF 中，通常会为一个 prompt 生成 K 个不同的回答，然后用奖励模型（Reward Model）为这些回答打分。传统的优势估计会在整个 batch 上进行归一化，会导致一个 prompt 中的高分回答拉高了整个 batch 的奖励基线，从而可能不公平地惩罚了另一个 prompt 的不错回答。GRPO 在组内（同一个 prompt 的 K 个回答中）进行奖励归一化，计算出的优势信号更稳定、更具有局部对比性\n",
    "2. KL 散度作为显式的惩罚项\n",
    "    在标准的 RLHF-PPO 流程中，为了防止策略模型 $\\pi_\\theta$ 偏离 SFT 的参考模型 $\\pi_{\\text{ref}}$ 太远，通常会将 KL 散度作为惩罚项加入到奖励信号中，即 $R = R_{\\text{RM}} - \\beta \\cdot \\text{RL}(\\pi_{\\theta}\\Vert\\pi_{\\text{ref}})$。而在 GRPO 中，KL 散度被直接添加到策略优化目标函数的后面，作为一个独立的惩罚项，这使得优化目标更加清晰。\n",
    "\n",
    "    其目标函数为：\n",
    "    $$\n",
    "    \\hat{\\mathbb{E}}_{q \\sim P(Q), \\{o_i\\}_{i = 1}^G \\sim \\pi_{\\theta_{\\text{old}}}(o|q)} \\lbrack\\frac{1}{G}\\sum_{i = 1}^G \\frac{1}{|o_i|}\\sum_{t = 1}^{|o_i|} \\lbrace \\text{min}(\\frac{\\pi_\\theta(o_{i, t}|q, o_{i, < t})}{\\pi_{\\text{old}}(o_{i, t}|q, o_{i, < t})}\\hat{A}_{i ,t}, \\text{clip}(...)\\hat{A}_{i,t}) \\rbrace - \\beta D_{\\text{KL}}[\\pi_\\theta \\Vert \\pi_{\\text{ref}}] \\rbrack\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30e9363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO implementation\n",
    "\n",
    "import torch\n",
    "\n",
    "def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):\n",
    "    '''\n",
    "    计算每个 token 的对数概率\n",
    "\n",
    "    Args:\n",
    "        model：语言模型\n",
    "        input_ids: 完整的输入序列 [batch, seq_len]\n",
    "        attention_mask: 注意力掩码 [batch, seq_len] \n",
    "        logit_to_keep: 需要保留的 logits 数量（对应 completion 部分的长度）\n",
    "\n",
    "    Returns:    \n",
    "        torch.Tensor: 每个 token 的对数概率 [batch, logits_to_keep]\n",
    "    '''\n",
    "    # 1. 获取模型输出的 logits\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits  # [batch, seq_len, vocab_size]\n",
    "\n",
    "    # 2. 提取 completion 部分的 logits\n",
    "    # 由于 logits 是预测下一个 token 的概率分布，因此需要偏移一个位置\n",
    "    completion_logits = logits[:, -(logits_to_keep + 1):-1, :] # [batch, logits_to_keep, vocab_size]\n",
    "    completion_labels = input_ids[:, -logits_to_keep:]  # [batch, logits_to_keep]\n",
    "\n",
    "    # 3. 计算每个 token 的对数概率\n",
    "    log_probs = F.log_softmax(completion_logits, dim=-1) # [batch, logits_to_keep, vocab_size]，相当于对每个位置的 vocab 维度做 softmax\n",
    "    \n",
    "    # 4. 选择对应 token 的对数概率\n",
    "    per_token_logps = log_probs.gather(dim=-1, index=completion_labels.unsqueeze(-1)).squeeze(-1) # [batch, logits_to_keep]\n",
    "\n",
    "    return per_token_logps\n",
    "\n",
    "def compute_grpo_loss(self, model, inputs):\n",
    "    '''\n",
    "    Args:\n",
    "        model: 正在训练的策略模型\n",
    "        inputs: 包含 prompt, completion, advantages 等数据的字典\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 计算输出的策略损失\n",
    "    '''\n",
    "\n",
    "    # 1. 准备输入 ID 和 attention mask\n",
    "    prompt_ids, prompt_mask = inputs['prompt_ids'], inputs['prompt_mask']\n",
    "    completion_ids, completion_mask = inputs['completion_ids'], inputs['completion_mask']\n",
    "    input_ids = torch.cat([prompt_ids, completion_ids], dim = 1)\n",
    "    attention_mask = torch.cat([prompt_mask, completion_mask], dim = 1)\n",
    "\n",
    "    # 2. 计算当前策略模型对 completion 部分的每个 token 的对数概率\n",
    "    logits_to_keep = completion_ids.size(1)\n",
    "    per_token_logps = self. _get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)\n",
    "\n",
    "    # 3. 获取优势函数和旧策略的对数概率\n",
    "    advantages = inputs['advantages'] # [batch,]\n",
    "    old_per_token_logps = inputs['old_per_token_logps']\n",
    "\n",
    "    # 4. 计算新旧策略概率比\n",
    "    log_ratios1 = torch.exp(per_token_logps - old_per_token_logps) # [batch, logits_to_keep]\n",
    "\n",
    "    # 5. 计算截断后的概率比\n",
    "    log_ratios2 = torch.clamp(log_ratios1, 1 - self.clip_eps, 1+ self.clip_eps)\n",
    "\n",
    "    # 6. 计算两个目标项\n",
    "    surr1 = log_ratios1 * advantages.unsqueeze(-1) # [batch, logits_to_keep]\n",
    "    surr2 = log_ratios2 * advantages.unsqueeze(-1) # [batch, logits_to_keep]\n",
    "\n",
    "    per_token_loss = - torch.min(surr1, surr2) # [batch, logits_to_keep]\n",
    "\n",
    "    return per_token_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78e1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
