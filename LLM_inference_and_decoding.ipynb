{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5438229",
   "metadata": {},
   "source": [
    "# LLM 推理和解码策略\n",
    "\n",
    "> 解码策略决定了如何从模型输出的词汇表概率分布中选择下一个 token。不同的策略在生成文本的多样性、准确性和计算成本之间做出了不同的权衡。\n",
    "\n",
    "## Greedy Search（贪心搜索）\n",
    "\n",
    "贪心搜索是最简单直接的解码策略。在每个时间步，它都会选择当前概率最高的 token 作为输出，然后将这个 token 作为下个时间步的输入，继续生成。\n",
    "\n",
    "- 优点：\n",
    "  - 实现简单，计算速度快\n",
    "- 缺点：\n",
    "  - 容易陷入局部最优。在某个时间步选择的局部最优 token，可能会导致后续整个序列的质量下降\n",
    "  - 生成的文本缺乏多样性，往往是重复和确定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e298958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def greedy_search(model_logits, max_len=20, eos_token_id=2):\n",
    "    '''\n",
    "    Args:\n",
    "        model_logits (torch.Tensor): shape: (batch_size, seq_len, vocab_size)\n",
    "        max_len (int): 最大生成长度\n",
    "        eos_token_id (int): id of end-of-sentence token\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 生成的 token 序列\n",
    "    '''\n",
    "    batch_size = model_logits.size(0)\n",
    "\n",
    "    # 存储生成的 token 索引\n",
    "    generated_sequence = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "\n",
    "    # 模拟逐个 token 生成过程\n",
    "    for t in range(max_len):\n",
    "        \n",
    "        # 获取当前时间步的 logits\n",
    "        current_logits = model_logits[:, t, :]\n",
    "\n",
    "        # 计算概率分布\n",
    "        probs = F.softmax(current_logits, dim=-1)\n",
    "\n",
    "        # 选择概率最高的 token 索引\n",
    "        next_token = torch.argnax(probs, dim=-1)\n",
    "\n",
    "        # 将选择的词加入生成序列中\n",
    "        generated_sequence[:, t] = next_token\n",
    "\n",
    "        # 检查所有 batch 都遇到结束标记，遇到则提前停止\n",
    "        if (next_token == eos_token_id).all():\n",
    "            break\n",
    "    \n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5dbc1",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "\n",
    "束搜索是对贪心搜索的一种改进，它在一定程度上克服了局部最优的问题。\n",
    "\n",
    "- 核心思想：在每个时间步，保留一个束宽 `beam_size` 个概率最高的候选序列。在下一个时间步，会基于这 `beam_size` 个候选序列，分别生成下一个词，然后从所有可能的序列中，再次选出总概率最高的 `beam_size` 个，并不断重复这个过程。\n",
    "- 优点：生成序列质量比贪心搜索更高，它考虑了更广的搜索空间。\n",
    "- 缺点：\n",
    "  - 计算成本更高，是贪心搜索的 `beam_size` 倍。\n",
    "  - 仍然可能错过全局最优解。\n",
    "  - 生成的文本可能偏向高频、安全的短语，多样性依然有限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c50bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def beam_search(lm_prob, beam_size=3):\n",
    "    '''\n",
    "    Args:\n",
    "        lm_probs (torch.Tensor): 模型输出的概率张量，shape: (batch, seq_len, vocab_size)\n",
    "        beam_size (int): 束宽\n",
    "\n",
    "    Returns:\n",
    "        tuple: (序列索引，对应的对数概率)\n",
    "    '''\n",
    "\n",
    "    batch, seq_len, vocab_size = lm_prob.shape\n",
    "    \n",
    "    # 为了避免下溢出并且将连乘转化为连加，对概率取对数\n",
    "    log_lm_prob = torch.log(lm_prob)\n",
    "\n",
    "    # -- initalization --\n",
    "    # 取第一个时间步概率最高的 k 个 token 作为初始 beam\n",
    "    # shape: log_beam_prob: (batch, beam_size) indices: (batch, beam_size)\n",
    "    log_beam_prob, indices = log_lm_prob[:, 0, :].topk(beam_size, sorted=True)\n",
    "\n",
    "    # 将 indices 扩展一维，用于后续拼接\n",
    "    # indices: (batch, beam_size, 1)\n",
    "    indices = indices.unsqueeze(-1)\n",
    "\n",
    "    # 逐时间步扩展 Beam\n",
    "    for i in range(1, seq_len):\n",
    "        # 1. 扩展所有候选\n",
    "        # log_beam_prob: (batch, beam_size) -> (batch, beam_size, 1)\n",
    "        # log_lm_prob: (batch, vocab_size) -> (batch, 1, vocab_size)\n",
    "        # current_log_probs: (batch, beam_size, vocab_size)\n",
    "        current_log_probs = log_beam_prob.unsqueeze(-1) + log_lm_prob.unsqueeze(1)\n",
    "        # 2. 选取 top-k\n",
    "        # 将 beam_size 和 vocab_size 维度合并，方便选取 top-k\n",
    "        # current_log_probs: (batch, beam_size * vocab_size)\n",
    "        current_log_probs = current_log_probs.view(batch, -1)\n",
    "        log_beam_prob, topk_indices = current_log_probs.topk(beam_size, sorted=True)\n",
    "        # 3. 更新 indices\n",
    "        # 计算对应的 beam 索引和 token 索引\n",
    "        beam_indices = topk_indices // vocab_size  # (batch, beam_size)\n",
    "        token_indices = topk_indices % vocab_size  # (batch, beam_size)\n",
    "        # 根据 beam_indices 从之前的 indices 中选取对应的序列\n",
    "        # indices: (batch, beam_size, i)\n",
    "        selected_indices = torch.gather(indices, 1, beam_indices.unsqueeze(-1).expand(-1, -1, i))\n",
    "        # 拼接新的 token 索引\n",
    "        # indices: (batch, beam_size, i + 1)\n",
    "        indices = torch.cat([selected_indices, token_indices.unsqueeze(-1)], dim=-1)\n",
    "    return indices, log_beam_prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
